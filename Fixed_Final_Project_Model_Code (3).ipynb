{"nbformat": 4, "nbformat_minor": 0, "metadata": {"colab": {"provenance": []}, "kernelspec": {"name": "python3", "display_name": "Python 3"}, "language_info": {"name": "python"}}, "cells": [{"cell_type": "code", "execution_count": null, "metadata": {"id": "An53lin92uAz"}, "outputs": [], "source": ["!pip -q install transformers datasets evaluate rouge_score"]}, {"cell_type": "code", "source": ["from datasets import load_dataset, Dataset, DatasetDict\n", "from transformers import AutoTokenizer, EarlyStoppingCallback\n", "\n", "t5_base = \"google-t5/t5-base\"\n", "data = load_dataset(\"abisee/cnn_dailymail\", \"3.0.0\")\n", "tokenizer = AutoTokenizer.from_pretrained(t5_base)\n", "data"], "metadata": {"id": "AoMSlP2vBZT5", "colab": {"base_uri": "https://localhost:8080/"}, "outputId": "768cc1c8-60f2-4c39-c5b0-988cf4e0153d"}, "execution_count": null, "outputs": [{"output_type": "execute_result", "data": {"text/plain": ["DatasetDict({\n", "    train: Dataset({\n", "        features: ['article', 'highlights', 'id'],\n", "        num_rows: 287113\n", "    })\n", "    validation: Dataset({\n", "        features: ['article', 'highlights', 'id'],\n", "        num_rows: 13368\n", "    })\n", "    test: Dataset({\n", "        features: ['article', 'highlights', 'id'],\n", "        num_rows: 11490\n", "    })\n", "})"]}, "metadata": {}, "execution_count": 2}]}, {"cell_type": "code", "source": ["import pandas as pd\n", "train_percent, val_percent, test_percent = 0.8, 0.1, 0.1\n", "\n", "\n", "def load_data_sampled(max_samples=10000):\n", "  train_df = pd.DataFrame(data['train'])\n", "  test_df = pd.DataFrame(data['test'])\n", "  val_df = pd.DataFrame(data['validation'])\n", "  train_df = train_df.sample(int(max_samples * train_percent)).reset_index(drop=True)\n", "  test_df = test_df.sample(int(max_samples * test_percent)).reset_index(drop=True)\n", "  val_df = val_df.sample(int(max_samples * val_percent)).reset_index(drop=True)\n", "  return train_df, test_df, val_df\n", "\n", "train_df, test_df, val_df = load_data_sampled(max_samples=10000)\n", "\n", "train_dataset = Dataset.from_pandas(train_df)\n", "test_dataset = Dataset.from_pandas(test_df)\n", "val_dataset = Dataset.from_pandas(val_df)\n", "\n", "sampled_data = DatasetDict({\n", "    'train': train_dataset,\n", "    'test': test_dataset,\n", "    'validation': val_dataset\n", "})\n", "\n", "print(\"New DatasetDict rows and features below:\\n\")\n", "sampled_data"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "ikSGT14CKllA", "outputId": "23647190-012f-44f4-b612-2d8e7320fa07"}, "execution_count": null, "outputs": [{"output_type": "stream", "name": "stdout", "text": ["New DatasetDict rows and features below:\n", "\n"]}, {"output_type": "execute_result", "data": {"text/plain": ["DatasetDict({\n", "    train: Dataset({\n", "        features: ['article', 'highlights', 'id'],\n", "        num_rows: 8000\n", "    })\n", "    test: Dataset({\n", "        features: ['article', 'highlights', 'id'],\n", "        num_rows: 1000\n", "    })\n", "    validation: Dataset({\n", "        features: ['article', 'highlights', 'id'],\n", "        num_rows: 1000\n", "    })\n", "})"]}, "metadata": {}, "execution_count": 3}]}, {"cell_type": "code", "source": ["prefix = \"summarize: \"\n", "def preprocess_function(examples):\n", "    inputs = [prefix + str(article) for article in examples[\"article\"]]\n", "    model_inputs = tokenizer(inputs, max_length=1024, truncation=True, padding=True)\n", "\n", "    labels = tokenizer(examples[\"highlights\"], max_length=150, truncation=True, padding=True)\n", "\n", "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n", "    return model_inputs\n", "\n", "tokenized_data = sampled_data.map(preprocess_function, batched=True, remove_columns=sampled_data['train'].column_names)\n", "tokenized_data"], "metadata": {"id": "HolisYWC9dMn", "colab": {"base_uri": "https://localhost:8080/", "height": 272, "referenced_widgets": ["92dfda8c72e1432ea1ca961b988f27ca", "4fe19b409cf04d288ff24e2caa406127", "3e8efe0276b048afb4b5c324cfd49644", "dc50421a15cc4a2ca45ca20b8f17e3c6", "0e43118fb5724a3ab430dc9c1e01a6fb", "a71220b131d7449ca7a889eaa6a8766d", "dda00e8adf2f40c38597faf3e3cc9458", "347ee6b8bea14930b2952fe541423533", "d5cd8d10eb084e5ab5cec76f1b24c6b1", "2328fe53e099413ead01bcb6e2d0f9d5", "f9640425c1334180902e804262a5f8fc", "b598eabb16b14b43a5253cdbe9a2acfe", "f27ac1eb60144b3ea6dd6464851d2057", "0bafee8fe02645579be83ad5e43c28cd", "8b84d808b72c4124924c62913ec81154", "77200a465d9544a7b7c080674aff442f", "23c21613d898442297a645e51994fca0", "aafdaa612d784d3a99a928524ca5f3f0", "d876e349784b40fb87a9c26ba4909109", "24e43dfcae814dc7a6efdf6c4881a196", "ed043243641f42beb0f2ab05191dcd20", "217e3a0fdcd2497798bf79e222c64e77", "c8f8dec591f4453b918880853a4ebada", "c45042b4b80d4598a5df135761fdaccc", "d42a024993374643a9fe15cf1753a74f", "5cb7010a8a7c4bb081d849595c861c75", "08a2b103db464dbeaca085f023172dea", "358bd0bc2a1f4938ae242f66655d609f", "f07f715a5de945d2af0ff17f6594ce91", "172461574fb14ddc9a0eef28bc81c06f", "8d876aa9aaf94344ae7511a2666f3aa4", "19d5ce78a9764dfaa84703d4da2d6173", "2ab5719aade8427faf9ff752389854cf"]}, "outputId": "72161554-5558-431c-c35c-dd9afc96ffb1"}, "execution_count": null, "outputs": [{"output_type": "display_data", "data": {"text/plain": ["Map:   0%|          | 0/8000 [00:00<?, ? examples/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "92dfda8c72e1432ea1ca961b988f27ca"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": ["Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "b598eabb16b14b43a5253cdbe9a2acfe"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": ["Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "c8f8dec591f4453b918880853a4ebada"}}, "metadata": {}}, {"output_type": "execute_result", "data": {"text/plain": ["DatasetDict({\n", "    train: Dataset({\n", "        features: ['input_ids', 'attention_mask', 'labels'],\n", "        num_rows: 8000\n", "    })\n", "    test: Dataset({\n", "        features: ['input_ids', 'attention_mask', 'labels'],\n", "        num_rows: 1000\n", "    })\n", "    validation: Dataset({\n", "        features: ['input_ids', 'attention_mask', 'labels'],\n", "        num_rows: 1000\n", "    })\n", "})"]}, "metadata": {}, "execution_count": 4}]}, {"cell_type": "code", "source": ["from transformers import DataCollatorForSeq2Seq\n", "import evaluate\n", "\n", "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=t5_base, label_pad_token_id=-100)\n", "rouge_metric = evaluate.load(\"rouge\")"], "metadata": {"id": "jxUeL8pA-xKv"}, "execution_count": null, "outputs": []}, {"cell_type": "code", "source": ["import numpy as np\n", "\n", "def compute_metrics(eval_preds):\n", "  predictions, labels = eval_preds\n", "  decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n", "  labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n", "  decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n", "\n", "  result = rouge_metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n", "  prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n", "  result[\"gen_len\"] = np.mean(prediction_lens)\n", "  return {k: round(v, 4) for k, v in result.items()}"], "metadata": {"id": "nzHll_Oy-y7q"}, "execution_count": null, "outputs": []}, {"cell_type": "code", "source": ["import torch\n", "from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n", "\n", "model = AutoModelForSeq2SeqLM.from_pretrained(\n", "    t5_base,\n", "    device_map=\"auto\",\n", "    torch_dtype=torch.bfloat16,\n", ")\n", "model.to(\"cuda\")\n", "\n", "model.gradient_checkpointing_enable(\n", "    gradient_checkpointing_kwargs={\"use_reentrant\": False}\n", ")\n", "model.zero_grad()"], "metadata": {"id": "cvCn2SE7CVWB", "colab": {"base_uri": "https://localhost:8080/"}, "outputId": "13591a40-dedd-497f-92b9-d945f4787547"}, "execution_count": null, "outputs": [{"output_type": "stream", "name": "stdout", "text": ["WARNING:tensorflow:From C:\\Users\\Sap Yap\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n", "\n"]}]}, {"cell_type": "code", "source": ["training_arguments = Seq2SeqTrainingArguments(\n", "    output_dir=\"fine-tuned-t5-cnn-dailymail-10000\",\n", "    num_train_epochs=6,\n", "    per_device_train_batch_size=16,\n", "    per_device_eval_batch_size=64,\n", "    warmup_steps=200,\n", "    logging_steps=100,\n", "    weight_decay=0.03,\n", "    learning_rate = 8e-5,\n", "    logging_dir='./logs',\n", "    eval_strategy=\"epoch\",\n", "    save_strategy=\"epoch\",\n", "    load_best_model_at_end=True,\n", "    metric_for_best_model=\"rouge2\",\n", "    save_total_limit=3,\n", "    predict_with_generate=True,\n", "    generation_max_length=150,\n", "    generation_num_beams=5,\n", "    bf16=True,\n", ")\n", "\n", "trainer = Seq2SeqTrainer(\n", "    model=model,\n", "    args=training_arguments,\n", "    train_dataset=tokenized_data[\"train\"],\n", "    eval_dataset=tokenized_data[\"validation\"],\n", "    data_collator=data_collator,\n", "    compute_metrics=compute_metrics,\n", "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n", ")\n", "\n", "trainer.train()"], "metadata": {"id": "L9V_8b-o1rJ6"}, "execution_count": null, "outputs": []}, {"cell_type": "code", "source": ["output_dir = \"fine-tuned-t5-cnn-dailymail-model\"\n", "trainer.save_model(output_dir)\n", "tokenizer.save_pretrained(output_dir)"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "9vv1pEANmayn", "outputId": "1ab10807-44e6-4f3f-e785-8962cc8c05f3"}, "execution_count": null, "outputs": [{"output_type": "execute_result", "data": {"text/plain": ["('fine-tuned-t5-cnn-dailymail-model\\\\tokenizer_config.json',\n", " 'fine-tuned-t5-cnn-dailymail-model\\\\special_tokens_map.json',\n", " 'fine-tuned-t5-cnn-dailymail-model\\\\tokenizer.json')"]}, "metadata": {}, "execution_count": 10}]}, {"cell_type": "code", "source": ["# load the now fine-tuned model\n", "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n", "\n", "model_path = \"ramyj/fine-tuned-t5-cnn-dailymail\"\n", "tokenizer = AutoTokenizer.from_pretrained(model_path)\n", "model = AutoModelForSeq2SeqLM.from_pretrained(model_path)"], "metadata": {"id": "RqW3L_NfRU9m"}, "execution_count": null, "outputs": []}, {"cell_type": "code", "source": ["import textwrap\n", "# You can customize this via slicing to how many you wanna test\n", "sample_articles = sampled_data[\"test\"][\"article\"][:1]\n", "sample_highlights = sampled_data[\"test\"][\"highlights\"][:1]\n", "prefix = \"summarize: \"\n", "\n", "\n", "for i, (article, reference) in enumerate(zip(sample_articles, sample_highlights)):\n", "    print(f\"Article {i+1}\")\n", "    print(\"=\" * 80)\n", "\n", "    # Prepare input for the model (single article)\n", "    input_text = prefix + article\n", "    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=1024, truncation=True)\n", "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n", "\n", "    # Generate summary\n", "    output = model.generate(\n", "        **inputs,\n", "        max_length=150,\n", "        min_length=30,\n", "        num_beams=5,\n", "        length_penalty=1.0,\n", "        early_stopping=True\n", "    )\n", "\n", "    # Decode the generated output\n", "    decoded_output = tokenizer.batch_decode(output, skip_special_tokens=True)[0]\n", "\n", "    print(\"\\nGENERATED SUMMARY:\")\n", "    # Format the generated summary with line breaks every ~80 characters\n", "    formatted_summary = textwrap.fill(decoded_output, width=80).replace(\" .\", \".\")\n", "    print(formatted_summary)\n", "\n", "    print(\"\\nREFERENCE SUMMARY:\")\n", "    # Format the reference summary with line breaks every ~80 characters\n", "    formatted_reference = textwrap.fill(reference, width=80).replace(\" .\", \".\") + \".\"\n", "    print(formatted_reference)\n", "\n", "    print(\"\\n\" + \"-\" * 80 + \"\\n\")"], "metadata": {"id": "WfEP2rpwmdwF", "colab": {"base_uri": "https://localhost:8080/"}, "outputId": "9f216053-b84c-4875-bdbe-88e2f1524b27"}, "execution_count": null, "outputs": [{"output_type": "stream", "name": "stdout", "text": ["Article 1\n", "================================================================================\n", "\n", "GENERATED SUMMARY:\n", "Eden Hazard won the PFA Player of the Year award at the Grosvenor Hotel in\n", "London. The 24-year-old has scored 18 goals across all competitions this season\n", ". Hazard joined Chelsea from Lille in 2012 and put pen to paper on a new five-\n", "and-a-half-year deal.\n", "\n", "REFERENCE SUMMARY:\n", "Eden Hazard has been voted the Player of the Year by his fellow professionals.\n", "The Belgian has been in glittering form for Chelsea, scoring 18 Premier League\n", "goals so far. Diego Costa, David de Gea, Alexis Sanchez, Harry Kane and\n", "Phillipe Coutinho were also nominees for the award. Hazard has helped Chelsea\n", "to win the Capital One Cup and the club are on the brink of the Premier League\n", "title. The 24-year-old received the Young Player gong last season - an award\n", "won by Spurs' Harry Kane this time out. Jose Mourinho: Hazard is worth \u00c2\u00a3100m\n", "for each leg PLUS Cristiano Ronaldo..\n", "\n", "--------------------------------------------------------------------------------\n", "\n"]}]}]}